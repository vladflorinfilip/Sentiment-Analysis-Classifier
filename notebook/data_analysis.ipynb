{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0211be-79cd-4e55-915e-be59864c37a7",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook is a project building a sentiment classifier for text of 10,000 movie reviews from IMDb. The movie reviews are classified in 48.22% **positive** and 50.78% **negative** sentiment classes. The smallest and longest reviews have a 6 and 1307 character lengths, respectively. The project is split in two parts:\n",
    "- The first part uses a Neurol Network built using python TensorFlow library:\n",
    "    - Using Dropout methods to counter overfitting\n",
    "    - Using L2 regularization techniques\n",
    "- The second part uses a Naive Bayes approach to create a sentiment classifier model\n",
    "\n",
    "## 1. TensorFlow Neural Network\n",
    "### 1.1 Database Processing and Analysis\n",
    "For this project, I used a 10,000 movie review database categoriezed into **positive** and **negative** reviews. The data was split into 60-20-20% sets for training, cross validation and testing, respectively. All letters for converted to lowercase letters and non-alphanumeric characters (e.g., punctuation) were scrapped because they are not useful in releaving any sentiment about the movie review. To speed up the training process, data reviews longer than 200 words (i.e., about 37.1% of the total set) were truncated. The **positive** and **negative** lables were converted to a binary set of 1s and 0s. The reviews were suffled and the ammount of **positive** and **negative** reviews in each of the three sets was cross-checked to avoid skewed datasets. Finnally, the text of the reviews was tokenized using the `preprocessing.text.Tokenizer` from the `keras` library. This created an index dictory for all unique words with the most frequently used having a lower index. The dictionary was then used to convert the reviews' text to a vectorized form used for processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8499e-a403-430c-81b0-9c93487296fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda83f64-4120-4ce7-be9e-e81a28072375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text= text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "    \n",
    "def load_and_preprocess_data(csv_file, max_words=10000, max_len=200):\n",
    "    \"\"\"\n",
    "    Load and preprocess the review data from CSV file with class balance verification\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file : str\n",
    "        Path to CSV file containing reviews and sentiments\n",
    "    max_words : int, optional (default=10000)\n",
    "        Maximum number of words to keep in vocabulary\n",
    "    max_len : int, optional (default=200)\n",
    "        Maximum length of each sequence\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : array-like\n",
    "        Padded sequences of reviews\n",
    "    y : array-like\n",
    "        Binary sentiment labels\n",
    "    tokenizer : Tokenizer\n",
    "        Fitted tokenizer object\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    print(f\"\\nLoading data from {csv_file}...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    total_samples = len(df)\n",
    "    print(f\"Total number of reviews: {total_samples:,}\")\n",
    "    \n",
    "    # Check class distribution before conversion\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    print(\"\\nOriginal class distribution:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count/total_samples) * 100\n",
    "        print(f\"{sentiment}: {count:,} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "    # Convert sentiment to binary\n",
    "    df['sentiment'] = (df['sentiment'] == 'Positive').astype(int)\n",
    "\n",
    "    # Verify expected class balance\n",
    "    n_positive = (df['sentiment'] == 1).sum()\n",
    "    n_negative = (df['sentiment'] == 0).sum()\n",
    "    if n_positive != 5000 or n_negative != 5000:\n",
    "        print(\"\\nWARNING: Unexpected class distribution!\")\n",
    "        print(f\"Expected: 5,000 positive and 5,000 negative\")\n",
    "        print(f\"Found: {n_positive:,} positive and {n_negative:,} negative\")\n",
    "    \n",
    "    # Get text length statistics\n",
    "    df['review_length'] = df['review'].str.len()\n",
    "    print(\"\\nReview length statistics:\")\n",
    "    print(f\"Mean length: {df['review_length'].mean():.1f} characters\")\n",
    "    print(f\"Median length: {df['review_length'].median():.1f} characters\")\n",
    "    print(f\"Max length: {df['review_length'].max():,} characters\")\n",
    "    print(f\"Min length: {df['review_length'].min():,} characters\")\n",
    "    \n",
    "    # Remove special characters and set all to lower case\n",
    "    df['review'] = df['review'].apply(clean_text)\n",
    "\n",
    "    # Initialize and fit tokenizer\n",
    "    print(\"\\nTokenizing reviews...\")\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df['review'])\n",
    "    \n",
    "    # Get vocabulary statistics\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "    print(f\"Total unique words: {vocab_size:,}\")\n",
    "    print(f\"Keeping top {max_words:,} words\")\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(df['review'])\n",
    "    \n",
    "    # Get sequence length statistics before padding\n",
    "    seq_lengths = [len(seq) for seq in sequences]\n",
    "    print(\"\\nSequence length statistics (before padding):\")\n",
    "    print(f\"Mean length: {np.mean(seq_lengths):.1f} words\")\n",
    "    print(f\"Median length: {np.median(seq_lengths):.1f} words\")\n",
    "    print(f\"Max length: {max(seq_lengths):,} words\")\n",
    "    print(f\"Min length: {min(seq_lengths):,} words\")\n",
    "    \n",
    "    # Calculate how many sequences will be truncated\n",
    "    n_truncated = sum(len(seq) > max_len for seq in sequences)\n",
    "    if n_truncated > 0:\n",
    "        print(f\"\\nWARNING: {n_truncated:,} reviews ({(n_truncated/total_samples)*100:.1f}%) \"\n",
    "              f\"will be truncated to {max_len} words\")\n",
    "    \n",
    "    # Pad sequences\n",
    "    print(f\"\\nPadding sequences to length {max_len}...\")\n",
    "    X = pad_sequences(sequences, maxlen=max_len)\n",
    "    y = df['sentiment'].values\n",
    "    \n",
    "    # Final verification of processed data\n",
    "    print(\"\\nFinal processed data shape:\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"Class balance in processed data: {np.mean(y)*100:.1f}% positive\")\n",
    "    \n",
    "    return X, y, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722633de-b497-4862-a992-098ddf34756e",
   "metadata": {},
   "source": [
    "### 1.2 Creating the Model\n",
    "One of the main concerns with creating the neural network model was to avoid overfitting: good performance on the trainning set and poor performance on unseen data. This is way the cross-validation set is used to help decide the paramaters used in creating the model. Two approaches are explored to counteract overfitting:\n",
    "- Dropout layer to turn off random neuron units\n",
    "- L2 regularization to adjust the weights within the neural layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e51dc-f5ac-4587-861b-f865632fc5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_words, max_len, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Create and compile the neural network model\n",
    "    \"\"\"\n",
    "    # Create Adam optimizer with recommended parameters\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=5e-5,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7,\n",
    "        amsgrad=False,\n",
    "        clipvalue=1.0\n",
    "    )\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim = max_words, output_dim = embedding_dim, input_length=max_len),\n",
    "        LSTM(64), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=adam_optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65043595-0097-4a0b-8fe9-e9b2f0959235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
